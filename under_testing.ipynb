{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download file... data.zip ...\n",
      "File downloaded\n",
      "All the files are downloaded\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "#from tqdm import tqdm\n",
    "from zipfile import ZipFile\n",
    "\n",
    "def download(url, file):\n",
    "    if not os.path.isfile(file):\n",
    "        print(\"Download file... \" + file + \" ...\")\n",
    "        urlretrieve(url,file)\n",
    "        print(\"File downloaded\")\n",
    "\n",
    "download('https://s3.amazonaws.com/video.udacity-data.com/topher/2016/December/584f6edd_data/data.zip','data.zip')\n",
    "#download('https://github.com/udacity/CarND-Traffic-Sign-Classifier-Project/blob/master/signnames.csv','signnames.csv')\n",
    "\n",
    "print(\"All the files are downloaded\")\n",
    "\n",
    "def uncompress_features_labels(dir,name):\n",
    "    if(os.path.isdir(name)):\n",
    "        print('Data extracted')\n",
    "    else:\n",
    "        with ZipFile(dir) as zipf:\n",
    "            zipf.extractall('data')\n",
    "uncompress_features_labels('data.zip','data')\n",
    "#uncompress_features_labels('data1.zip','data1')\n",
    "\n",
    "\n",
    "def data_Files(mypath):\n",
    "    onlyfiles = [f for f in os.listdir(mypath) if os.path.isfile(os.path.join(mypath, f))]\n",
    "    print(onlyfiles)\n",
    "\n",
    "#data_Files('datalab')\n",
    "#data_Files('signnames.csv')\n",
    "#data_Files('data1')\n",
    "#!ls\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6830, 7)\n",
      "['IMG/center_2016_12_01_13_32_50_019.jpg', ' IMG/left_2016_12_01_13_32_50_019.jpg', ' IMG/right_2016_12_01_13_32_50_019.jpg', ' 0', ' 0.9855326', ' 0', ' 30.18692']\n",
      "(1206, 7)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Flatten, Activation, Dropout\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers import Lambda, Cropping2D\n",
    "from tqdm import tqdm\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "samples = []\n",
    "with open('./data/data/driving_log.csv') as csvfile:\n",
    "    reader = csv.reader(csvfile)\n",
    "    next(reader, None) \n",
    "    for line in reader:\n",
    "        samples.append(line)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_samples, validation_samples = train_test_split(samples,test_size=0.15)\n",
    "print(np.shape(train_samples))\n",
    "print(train_samples[0])\n",
    "print(np.shape(validation_samples))\n",
    "#train_samples = len(train_samples)*6\n",
    "#validation_samples = len(validation_samples)*6\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generator(samples, batch_size=32):\n",
    "    num_samples = len(samples)\n",
    "   \n",
    "    while 1: \n",
    "        shuffle(samples)\n",
    "        for offset in range(0, num_samples, batch_size):\n",
    "            batch_samples = samples[offset:offset+batch_size]\n",
    "\n",
    "            images = []\n",
    "            angles = []\n",
    "            for batch_sample in batch_samples:\n",
    "                    for i in range(0,3):\n",
    "                        name = './data/data/IMG/'+batch_sample[i].split('/')[-1]\n",
    "                        \n",
    "                        center_image = cv2.cvtColor(cv2.imread(name), cv2.COLOR_BGR2RGB)\n",
    "                        center_angle = float(batch_sample[3])\n",
    "                        images.append(center_image)\n",
    "                        if(i==0):\n",
    "                            angles.append(center_angle)\n",
    "                        elif(i==1):\n",
    "                            angles.append(center_angle+0.2)\n",
    "                        elif(i==2):\n",
    "                            angles.append(center_angle-0.2)\n",
    "                        \n",
    "                        images.append(cv2.flip(center_image,1))\n",
    "                        if(i==0):\n",
    "                            angles.append(center_angle*-1)\n",
    "                        elif(i==1):\n",
    "                            angles.append((center_angle+0.2)*-1)\n",
    "                        elif(i==2):\n",
    "                            angles.append((center_angle-0.2)*-1)\n",
    "                            \n",
    "                        \n",
    "                    \n",
    "               \n",
    "            # trim image to only see section with road\n",
    "        \n",
    "            X_train = np.array(images)\n",
    "            y_train = np.array(angles)\n",
    "            #print(np.shape(X_train))\n",
    "            #print(np.shape(y_train))\n",
    "            yield sklearn.utils.shuffle(X_train, y_train)\n",
    "\n",
    "# compile and train the model using the generator function\n",
    "train_generator = generator(train_samples, batch_size=32)\n",
    "validation_generator = generator(validation_samples, batch_size=32)\n",
    "\n",
    "#print(np.shape(train_generator))\n",
    "#print(np.shape(validation_generator))\n",
    "\n",
    "ch, row, col = 3, 80, 320  # Trimmed image format\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6830, 7)\n",
      "Epoch 1/5\n",
      "6720/6830 [============================>.] - ETA: 0s - loss: 0.0586"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/carnd/anaconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py:1569: UserWarning: Epoch comprised more than `samples_per_epoch` samples, which might affect learning results. Set `samples_per_epoch` correctly to avoid this warning.\n",
      "  warnings.warn('Epoch comprised more than '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6912/6830 [==============================] - 13s - loss: 0.0580 - val_loss: 0.0271\n",
      "Epoch 2/5\n",
      "6912/6830 [==============================] - 12s - loss: 0.0276 - val_loss: 0.0182\n",
      "Epoch 3/5\n",
      "6912/6830 [==============================] - 12s - loss: 0.0251 - val_loss: 0.0214\n",
      "Epoch 4/5\n",
      "6912/6830 [==============================] - 12s - loss: 0.0263 - val_loss: 0.0224\n",
      "Epoch 5/5\n",
      "6996/6830 [==============================] - 13s - loss: 0.0227 - val_loss: 0.0169\n",
      "Done\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "lambda_10 (Lambda)               (None, 160, 320, 3)   0           lambda_input_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "cropping2d_10 (Cropping2D)       (None, 65, 320, 3)    0           lambda_10[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_46 (Convolution2D) (None, 31, 158, 24)   1824        cropping2d_10[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_73 (Activation)       (None, 31, 158, 24)   0           convolution2d_46[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_47 (Convolution2D) (None, 14, 77, 36)    21636       activation_73[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_74 (Activation)       (None, 14, 77, 36)    0           convolution2d_47[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_48 (Convolution2D) (None, 5, 37, 48)     43248       activation_74[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_75 (Activation)       (None, 5, 37, 48)     0           convolution2d_48[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_49 (Convolution2D) (None, 3, 35, 64)     27712       activation_75[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_76 (Activation)       (None, 3, 35, 64)     0           convolution2d_49[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_50 (Convolution2D) (None, 1, 33, 64)     36928       activation_76[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_77 (Activation)       (None, 1, 33, 64)     0           convolution2d_50[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_10 (Flatten)             (None, 2112)          0           activation_77[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_37 (Dense)                 (None, 100)           211300      flatten_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)              (None, 100)           0           dense_37[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "activation_78 (Activation)       (None, 100)           0           dropout_2[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_38 (Dense)                 (None, 50)            5050        activation_78[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_79 (Activation)       (None, 50)            0           dense_38[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_39 (Dense)                 (None, 10)            510         activation_79[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "activation_80 (Activation)       (None, 10)            0           dense_39[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_40 (Dense)                 (None, 1)             11          activation_80[0][0]              \n",
      "====================================================================================================\n",
      "Total params: 348,219\n",
      "Trainable params: 348,219\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "# Preprocess incoming data, centered around zero with small standard deviation \n",
    "#model.add(Lambda(lambda x: x/127.5 - 1., input_shape=(3, 160, 320), output_shape=(3, 80, 320)))\n",
    "model.add(Lambda(lambda x: (x / 127.5) - 1, input_shape=(160,320,3)))\n",
    "model.add(Cropping2D(cropping=((70,25),(0,0))))\n",
    "\n",
    "model.add(Convolution2D(24,5,5,subsample=(2,2)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Convolution2D(36,5,5,subsample=(2,2)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Convolution2D(48,5,5,subsample=(2,2)))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Convolution2D(64,3,3))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Convolution2D(64,3,3))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.add(Dense(100))\n",
    "\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('elu'))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(loss='mse',optimizer='adam')\n",
    "#model.fit(X_train,y_train,nb_epoch=5,validation_split=0.2,shuffle=True)\n",
    "\n",
    "print(np.shape(train_samples))\n",
    "\n",
    "#model.fit_generator(train_generator, samples_per_epoch= len(train_samples), validation_data=validation_generator, validation_steps=len(validation_samples), nb_epoch=3,verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "model.fit_generator(train_generator, samples_per_epoch= len(train_samples), validation_data=validation_generator,   nb_val_samples=len(validation_samples), nb_epoch=5, verbose=1)\n",
    "model.save('model.h5')\n",
    "print('Done')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
